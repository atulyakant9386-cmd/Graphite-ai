{"@context":{"@language":"en","@vocab":"https://schema.org/","citeAs":"cr:citeAs","column":"cr:column","conformsTo":"dct:conformsTo","cr":"http://mlcommons.org/croissant/","data":{"@id":"cr:data","@type":"@json"},"dataBiases":"cr:dataBiases","dataCollection":"cr:dataCollection","dataType":{"@id":"cr:dataType","@type":"@vocab"},"dct":"http://purl.org/dc/terms/","extract":"cr:extract","field":"cr:field","fileProperty":"cr:fileProperty","fileObject":"cr:fileObject","fileSet":"cr:fileSet","format":"cr:format","includes":"cr:includes","isEnumeration":"cr:isEnumeration","isLiveDataset":"cr:isLiveDataset","jsonPath":"cr:jsonPath","key":"cr:key","md5":"cr:md5","parentField":"cr:parentField","path":"cr:path","personalSensitiveInformation":"cr:personalSensitiveInformation","recordSet":"cr:recordSet","references":"cr:references","regex":"cr:regex","repeated":"cr:repeated","replace":"cr:replace","sc":"https://schema.org/","separator":"cr:separator","source":"cr:source","subField":"cr:subField","transform":"cr:transform","wd":"https://www.wikidata.org/wiki/"},"alternateName":"New GGMLv3 format for breaking llama.cpp change May 19th commit 2d5db48","conformsTo":"http://mlcommons.org/croissant/1.0","license":{"@type":"sc:CreativeWork","name":"Other (specified in description)"},"distribution":[{"contentUrl":"https://www.kaggle.com/api/v1/datasets/download/lorentzyeung/llama-7b-ggmlv3-q4-0-bin?datasetVersionNumber=1","contentSize":"3.385 GB","md5":"roYCOMANEiD10yDdPiYI8w==","encodingFormat":"application/zip","@id":"archive.zip","@type":"cr:FileObject","name":"archive.zip","description":"Archive containing all the contents of the llama-7b.ggmlv3.q4_0.bin dataset"},{"includes":"*.json","containedIn":{"@id":"archive.zip"},"encodingFormat":"application/json","@id":"application-json_fileset","@type":"cr:FileSet","name":"application/json files","description":"application/json files contained in archive.zip"}],"version":1,"keywords":["subject \u003E science and technology \u003E computer science \u003E programming","subject \u003E science and technology \u003E computer science","analysis \u003E nlp","subject \u003E culture and humanities \u003E languages","language \u003E english"],"isAccessibleForFree":true,"includedInDataCatalog":{"@type":"sc:DataCatalog","name":"Kaggle","url":"https://www.kaggle.com"},"creator":{"@type":"sc:Person","name":"Lorentz","url":"/lorentzyeung","image":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1049001-kg.jpg?t=2023-09-23-13-38-03"},"publisher":{"@type":"sc:Organization","name":"Kaggle","url":"https://www.kaggle.com/organizations/kaggle","image":"https://storage.googleapis.com/kaggle-organizations/4/thumbnail.png"},"thumbnailUrl":"https://storage.googleapis.com/kaggle-datasets-images/3817671/6615371/2e15e5d7c71053c45580d8d11c5e5b8d/dataset-card.jpg?t=2023-10-05-11-36-41","dateModified":"2023-10-05T11:34:30.85","datePublished":"2023-10-05T11:34:30.85","@type":"sc:Dataset","name":"llama-7b.ggmlv3.q4_0.bin","url":"https://www.kaggle.com/datasets/lorentzyeung/llama-7b-ggmlv3-q4-0-bin/versions/1","description":"**Git LFS Details**\n\n[Origin: https://huggingface.co/TheBloke/LLaMa-7B-GGML](https://huggingface.co/TheBloke/LLaMa-7B-GGML)\nSHA256: bcb95f6755597f26046ab2d5ebea51bf1418f440a96e1563f0fecc379c2cbee3\nPointer size: 135 Bytes\nSize of remote file: 3.79 GB\n\n[Raw pointer file](https://huggingface.co/TheBloke/LLaMa-7B-GGML/raw/main/llama-7b.ggmlv3.q4_K_S.bin)\n\nGit Large File Storage (LFS) replaces large files with text pointers inside Git, while storing the file contents on a remote server.[ More info](https://git-lfs.github.com/).\n\n**Meta\u0027s LLaMA 7b GGML**\nThese files are GGML format model files for [Meta\u0027s LLaMA 7b](https://ai.meta.com/blog/large-language-model-llama-meta-ai).\n\nGGML files are for CPU \u002B GPU inference using llama.cpp and libraries and UIs which support this format, such as:\n\n**KoboldCpp**, a powerful GGML web UI with full GPU acceleration out of the box. Especially good for story telling.\n**LoLLMS Web UI,** a great web UI with GPU acceleration via the c_transformers backend.\n**LM Studio**, a fully featured local GUI. Supports full GPU accel on macOS. Also supports Windows, without GPU accel.[](url)\n**text-generation-webui**, the most popular web UI. Requires extra steps to enable GPU accel via llama.cpp backend.\n**ctransformers**, a Python library with LangChain support and OpenAI-compatible AI server.\n**llama-cpp-python**, a Python library with OpenAI-compatible API server.\nThese files were quantised using hardware kindly provided by[ Latitude.sh](https://www.latitude.sh/accelerate).\n\n**Repositories available**\n[GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/LLaMA-7b-GPTQ)\n[2, 3, 4, 5, 6 and 8-bit GGML models for CPU\u002BGPU inference](https://huggingface.co/TheBloke/LLaMA-7b-GGML)\n[Unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: None](https://huggingface.co/huggyllama/llama-7b)\n\n**Compatibility**\n**Original llama.cpp quant methods:** q4_0, q4_1, q5_0, q5_1, q8_0\nThese are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.\n\n**New k-quant methods:** q2_K, q3_K_S, q3_K_M, q3_K_L, q4_K_S, q4_K_M, q5_K_S, q6_K\nThese new quantisation methods are compatible with llama.cpp as of June 6th, commit 2d43387.\n\nThey are now also compatible with recent releases of text-generation-webui, KoboldCpp, llama-cpp-python, ctransformers, rustformers and most others. For compatibility with other tools and libraries, please check their documentation.\n\n"}